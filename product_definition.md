# Multi‑User Personal Data Chatbot on Google Cloud : Product Definition

## Introduction  
Individuals today collect diverse personal datasets – from chat messages and health metrics to location trails and music listening habits. Harnessing this data for insights can be daunting without technical skills. This document outlines a **multi-user SaaS chatbot application** that enables users to query their own data in natural language. The proposed solution combines a secure, multi-tenant **Google Cloud Platform (GCP)** architecture with Large Language Models (LLMs) for intelligent query parsing and response generation. The chatbot will parse complex questions (e.g. distinguishing “last year” from “in the last 12 months”), fetch and join data across a user’s datasets, and deliver coherent answers or visualisations. It uses Google’s latest Vertex AI LLMs for reasoning and summarising results, all while adapting to each user’s available data and respecting privacy. In the following sections, we present the technical architecture, key interaction flows with storyboard-style mockups, a detailed product definition (personas, use cases, features, security), a phased development roadmap, and example prompts with a ([How to use RAG in BigQuery to bolster LLMs | Google Cloud Blog](https://cloud.google.com/blog/products/data-analytics/how-to-use-rag-in-bigquery-to-bolster-llms#:~:text=1)) ([How to use RAG in BigQuery to bolster LLMs | Google Cloud Blog](https://cloud.google.com/blog/products/data-analytics/how-to-use-rag-in-bigquery-to-bolster-llms#:~:text=BigQuery%20simplifies%20RAG%20in%20a,parts%20of%20it%20in%20a))style and rigour echo *The Economist* and *The Guardian*, aiming to inform a technical product owner with precision and clarity.

## Technical Architecture (GCP Platform and Data)  
**Overview:** The chatbot is built with GCP-native services for scalability, security, and integration with advanced AI. It follows a **multi-tenant architecture**: all users share the application and infrastructure, but each user’s data is isolated. The system dynamically discovers which data each user has provided and formulates queries accordingly. **Figure 1** illustrates a high-level multi-tenant design, where application workloads and data storage are compartmentalised per customer for security. Each user (tenant) shares the overall platform but has segregated data and access controls, ensuring their personal information remains invisible to others. 

![Figure 1: High-level multi-tenant architecture on GCP. Each customer’s application instance uses separate data stores (BigQuery, storage, etc.), enforced by Cloud IAM and optionally KMS for encryption. Shared services are minimised, and service accounts (SAs) are distinct per tenant to limit blast radius.](https://github.com/user-attachments/assets/a5c4ddd8-2900-4427-b297-51ca32f6abed)

### Data Ingestion and Storage  
Users upload structured datasets (e.g. CSV files of workouts or JSON exports of message logs) via a web interface. Uploaded files are first stored in **Cloud Storage** (for staging and backup) and then loaded into **BigQuery** tables. BigQuery is a serverless data warehouse well-suited for moderate table sizes (hundreds to thousands of records) and can handle up to 100k rows per table with ease. Each user gets a dedicated BigQuery *dataset* (a namespace for tables) under the hood – for example, *User123.Messages*, *User123.Fitness*, etc. This provides logical isolation; queries are always scoped to the user’s dataset. In a higher-security configuration, the application could even create a separate GCP project for each user’s data storage to achieve physical isolation by default. In either case, **access control** is enforced so that only the owning user (and the service operating on their behalf) can access their BigQuery tables. All data at rest is encrypted by GCP-managed keys, and for sensitive personal data, the system could use **Cloud KMS** with customer-managed keys for an extra layer of security.

Data schemas may vary slightly between users. For instance, one user’s cycling activity table might have columns `date, distance, route` while another’s has `timestamp, distance_km, duration`. Rather than hard-coding column names, the system inspects the schema of each dataset upon upload and stores metadata (field types, known synonyms, etc.). A small **metadata registry** (in BigQuery or Firestore) maps broad data categories to user-specific schema details. This allows the query planner to interpret a question like “How far did I cycle last week?” for any user by knowing which table and columns contain the cycling distance and timestamps for that user.

The architecture also includes an **optional semantic data store** for unstructured or textual data. For example, if users upload message histories or journal entries, the system generates vector embeddings for text fields and stores them for semantic search. Unstructured text entries (like message content) are converted into high-dimensional vectors using a text embedding model (via BigQuery ML or Vertex AI). BigQuery’s newer capabilities support storing and indexing vector embeddings directly, enabling similarity search within the warehouse. This means the chatbot can find relevant text snippets (e.g. a particular conversation) by semantic similarity, not just exact keywords. The **vector index** can be created in BigQuery for efficient retrieval, or a managed vector DB (such as Vertex AI Matching Engine or an open-source service) can be used if preferred. Consolidating structured data and embeddings in BigQuery simplifies governance by keeping all data queries in one platform, so the same access rules apply uniformly.

### Query Processing and LLM Orchestration  
At query time, the system leverages an orchestrator component (running on **Cloud Run** or **Google Kubernetes Engine**) that coordinates between the user interface, BigQuery, and the LLM. When a user asks a question in natural language, the following steps occur:

1. **NLU Parsing & Intent Recognition:** The user’s query is passed to an LLM (or a lightweight NLP parser) to interpret intent, entities, and temporal context. The LLM (such as Google’s PaLM 2 or Gemini model on Vertex AI) examines phrasing to infer what data is needed. For example, “How many messages did I send last year?” indicates the *Messages* dataset, a count metric, and a time filter for the previous calendar year. The parser also normalises relative dates (“last year” vs “in the last year”) into explicit date ranges. If parts of the question are ambiguous (“top songs… over what time period?”), the system may respond with a clarifying question to the user. This step uses a **Vertex AI** language model with a custom prompt that includes the user’s available data types and example interpretations. By using an LLM for this stage, the system can leverage its understanding of context and phrasing to map questions to data operations, much like natural language SQL generation.  

2. **Query Planning:** Based on the parsed intent, the orchestrator formulates a plan to retrieve the answer. This often involves constructing one or more SQL queries to BigQuery. For simple questions referencing a single dataset, one query may suffice (e.g. sum up steps in the given period). For complex questions, the planner may need to join data from multiple tables or perform sequential queries. Example: “During my vacation in France, did I burn more calories on days I listened to upbeat music?” might require filtering the *Location* dataset for entries in France (to identify “vacation” days), then filtering *Fitness* data for those days’ calories burned, and also checking the *Music* listening dataset for song tempo on those days. The orchestrator would likely run a series of queries: one to tag dates with location = France, another to aggregate fitness metrics on those dates, perhaps joining with a subquery or interim table of music tempo classifications. The query planner uses the metadata registry to adapt to each user’s schema, inserting the correct table and field names into the SQL. Complex query planning logic can be implemented with frameworks like **LangChain** agents or a custom planner. (Notably, LangChain’s SQL agent toolkit can connect an LLM to a database – in this case BigQuery via a SQLAlchemy driver – allowing the LLM to draft SQL statements autonomously. This approach was demonstrated by Harshad in using Vertex AI to interpret natural language and query BigQuery.)

3. **Data Retrieval:** The system executes the planned SQL queries on BigQuery. Thanks to BigQuery’s scalability, even multi-GB datasets can be processed in seconds, though here we deal with smaller personal datasets (under 100k rows). Query results are returned to the backend. If a semantic search was needed (for example, the question asks “What did I say about project X in my chats last month?”), the orchestrator uses the vector index: it converts the query or key phrases into an embedding and finds the nearest text entries (e.g. chat messages) via BigQuery’s vector search or an API call to the vector store. The relevant text snippets are retrieved alongside any structured data results. 

4. **LLM Answer Composition:** With the raw data (numbers, records, text excerpts) fetched, the system invokes the LLM again to compose a user-friendly answer. This step is performed by a **Vertex AI Generative model** (for instance, a PaLM 2 text-generation model or the newer Gemini model). The prompt to the LLM includes a concise summary of the question, the relevant data results, and instructions to formulate a helpful answer. If multiple partial results were obtained, the LLM can reason over them to produce insights – e.g. comparing metrics across two periods, or summarising a set of records. The use of **retrieval-augmented generation (RAG)** ensures that the LLM’s answer is grounded in the user’s actual data. The model effectively acts as an analyst, explaining or summarising what the data shows. For example, it might reply: “You sent **1,240 messages** in 2024, about 200 more than in 2023. The busiest month was June 2024.” The LLM is instructed to cite the data when appropriate and avoid fabricating information beyond what was retrieved. If the data is insufficient to answer (e.g. the user asks about a type of data they haven’t uploaded), the system responds gracefully, possibly with an apology or suggestion to provide that data. It is crucial that the chatbot **does not hallucinate** nonexistent data. In fact, a similar personal chatbot project explicitly chose to say it cannot answer a question if the data isn’t available, to reduce misinformation. Our system will follow this principle: answers are either derived from user data or clearly state if something cannot be determined.

5. **Response Delivery:** Finally, the composed answer (text, and possibly charts or tables) is sent back to the front-end chat interface for the user to view. The architecture ensures minimal latency by parallelising where possible – for instance, the query planning LLM call might predict multiple SQL queries at once, or the system might stream partial LLM answers. BigQuery’s fast query execution and Vertex AI’s hosted models (which are optimised for low-latency inference) support responsive interactions. The chat UI updates with the answer and may allow the user to ask a follow-up. Throughout the exchange, the conversation context (prior Q&A pairs) is maintained in short-term memory (in the backend or cached in the client) so that follow-up questions can be understood in context. For multi-turn dialogue, the orchestration includes a rolling conversation state: recent questions and answers are included in the prompt for step 1 and step 4 as needed, enabling the LLM to resolve references like “she” or “that week” based on earlier discussion.

### Scalability, Multitenancy, and Security Considerations  
The solution is designed for multi-tenant use from day one. This means all components enforce **data isolation** and user-specific context. BigQuery natively supports row-level security and dataset-level permissions, so each query executed includes the user’s credentials or a service account scoped to that user’s data. In practice, the backend service uses a *service account* with access to all user datasets, but it only queries the dataset corresponding to the active user’s ID. Alternatively, the system could employ **per-tenant service accounts** (as shown in Figure 1’s Cloud IAM layer), where each user is assigned a dedicated service identity that has access only to their data stores. This adds an extra safeguard: even if the application logic errs, the underlying credentials wouldn’t allow cross-tenant data access. Each approach aligns with the principle that *each tenant’s data is isolated and remains invisible to other tenants*. 

Scalability is addressed at multiple layers. **Cloud Run** can autoscale the stateless query orchestrator to handle concurrent users. **BigQuery** effortlessly scales with data volume and query load; given our moderate per-user data sizes, we are well within BigQuery’s comfort zone. The LLM calls to Vertex AI are the most expensive in terms of compute. Vertex AI endpoints scale based on traffic, and we can configure model size per request or use smaller models for lightweight tasks (like initial parsing) and larger models for final answer composition. For cost and performance optimisation, the system might cache frequent query results or schedule periodic computations (for example, pre-aggregating some statistics daily). However, due to personal data’s often real-time nature (e.g. yesterday’s steps count), caching is mostly useful for static analyses or expensive vector search results.

**Extensibility:** The architecture is data-agnostic – it does not hardcode any particular dataset schema or question type. New data connectors can be added for users to import additional types of information (for instance, integrating directly with Fitbit’s API to pull fitness data, or Google Takeout for location history). The core remains the same: store structured data in BigQuery (possibly with minor transformation), update the metadata registry with the new schema, and the LLM-based planner can immediately leverage it. The LLM’s broad knowledge of domains means it can handle unusual dataset questions as long as it has the schema and content. For example, if a user uploads a dataset of **reading history** (book titles with dates read and ratings), the system could handle questions like “How many pages did I read last month?” by identifying a “books” table and summing a “pages” field – assuming the metadata tells it such a field exists. We would test and refine the prompt engineering to ensure the LLM knows how to use new fields. Because Vertex AI continually improves and can be custom-prompted or fine-tuned, the system’s NLP capability will only get better at handling novel scenarios over time.

**Security & Privacy:** Personal data is highly sensitive, so the platform employs stringent security measures (detailed in a later section). In summary, all client connections are over HTTPS, user authentication is required (e.g. OAuth 2.0 login), and the backend strictly checks user identity on every request. Data in BigQuery is encrypted at rest by default, and access is audited. We also ensure that queries to the LLM do not inadvertently expose personal identifiers beyond what is necessary for context. Vertex AI, being a Google service, does not use customer-provided data to train its models unless explicitly enabled, so users’ private data remains private. Compliance considerations (GDPR, etc.) are taken into account: users can delete their data, and the system will purge it from storage (with BigQuery and Cloud Storage deletion, and any derived vector indices). Logging is careful not to record raw personal data; only system events and perhaps anonymised metrics are logged for monitoring.

In summary, the technical architecture marries **GCP’s data prowess** (BigQuery for structured and vector data) with **LLM intelligence** (Vertex AI) to create a flexible, robust personal data chatbot. It is built to be **multi-tenant and secure by design**, capable of scaling to many users while keeping each user’s digital life separate and safe. Next, we illustrate how users interact with the chatbot through key product flows and mockups.

## Key Interaction Flows & Chatbot Mockups  
To clarify the user experience, this section walks through key interactions with the system, accompanied by storyboard-style descriptions. The focus is on how a typical user would upload data and query it via the chatbot. Each flow highlights the dynamic between the user and the application (what the user sees or does, and how the system responds).

### 1. Onboarding and Data Ingestion Flow  
**Scenario:** A new user signs up and uploads their first dataset.  
1. **User Registration:** The user visits the SaaS application’s web portal and signs in (using a secure login, e.g. Google OAuth or email 2FA). They land on a welcome dashboard that explains how to add data for the chatbot to use.  
2. **Dataset Upload:** The user clicks “Add Data” and is presented with options to upload files or connect to services. In our example, the user chooses to upload a CSV of their past year’s fitness activities. A mockup of this interface shows a file picker and a form to describe the dataset (dataset name, type/category selection, etc.). The user names it “Cycling Workouts 2024” and selects the category “Fitness – Cycling”. They then upload the file.  
3. **Schema Detection:** The system processes the file: it detects columns like *date, distance, duration, calories*. The user is shown a preview: “We found 4 columns: Date, Distance (km), Duration (min), Calories”. The interface might ask for confirmation or data type adjustments (e.g. ensure Date is recognised as date). In this storyboard, the user confirms the schema looks correct.  
4. **Storage Confirmation:** The data is then sent to BigQuery (behind the scenes). After a brief processing time, the UI notifies the user that the dataset has been successfully added. The user now sees “Cycling Workouts 2024” listed in their Data Sources list on the dashboard. There is a small indicator that the data is **indexed and ready** for queries. (If this were a large text dataset, here we would also show progress of embedding creation, but for numeric data it’s instant.)  
5. **Continuous Update Option:** The interface also offers the user options to update this dataset later – e.g. upload a new file to overwrite, or schedule periodic appends. In our case, the user decides to later add their 2023 cycling data as well, which they can do by the same process, resulting in another table.

![Mockup Illustration:* *Frame 1:* A dashboard screen showing “No data added yet. Get started by uploading your personal data.” *Frame 2:* The file upload modal with fields for dataset name and category. *Frame 3:* A confirmation message: “✅ Cycling Workouts 2024 uploaded. 1,045 records added.” The data source list now contains an entry for this dataset.](https://github.com/user-attachments/assets/0f8ecc47-ed87-40c7-8a33-077663936448)

### 2. Asking a Simple Question (Single Dataset Query)  
**Scenario:** The user asks the chatbot a straightforward question about one of their datasets.  
1. **User’s Question:** The user navigates to the **Chatbot** section of the app. They are greeted by the chatbot interface – a text input box and a chat history pane. They type: *“How many kilometres did I cycle in total last year?”* and hit send.  
2. **Chatbot Understanding:** The chatbot (powered by the backend LLM and planner) identifies that “cycle” and “kilometres” likely relate to the **Cycling Workouts** dataset. It also interprets “last year” as the year 2024 (assuming current date is in 2025).  
3. **System’s Data Retrieval:** Without further ado, the system runs a query on the user’s *Cycling Workouts 2024* table, summing the distance column for the date range 2024-01-01 to 2024-12-31. It gets a result, say **3,560 km**.  
4. **Chatbot’s Answer:** The chatbot responds in the chat: *“You cycled **3,560 kilometres** in total during 2024.”* The answer appears within a few seconds of the user’s question. It is phrased in a friendly, factual tone. The interface might style the number in bold for clarity. If the user had a *Cycling Workouts 2023* dataset as well, the bot might even add: *“For comparison, in 2023 you logged 3,421 kilometres.”* (The chatbot is capable of slight embellishments like comparisons if relevant data is present, even if the user didn’t explicitly ask – though this is configurable to avoid overwhelming the user.)  
5. **User Acknowledgment:** The user sees the answer and is satisfied. They did not need to specify the dataset or dates explicitly – the system figured it out. The user might follow up with another question or just keep the information in mind. They now trust that they can ask more complex queries.

*Mockup Illustration:* A chat window screenshot: On the left, the user’s avatar next to their query: “**User:** How many kilometres did I cycle in total last year?”. Below it on the right, the chatbot avatar and answer: “**Chatbot:** You cycled **3,560 km** in total during 2024.” This simple Q&A demonstrates an MVP-level capability. The style is akin to messaging apps – clear distinction between user and bot messages, and the bot message perhaps with a small icon indicating it came from the “Personal Analyst Bot”. 

### 3. Clarification Dialogue (Ambiguity Resolution)  
**Scenario:** The user asks a question that is ambiguous or too broad, and the chatbot engages to clarify.  
1. **User’s Question:** The user now asks: *“What about last year?”* as a follow-up, without additional context. Since this question is vague on its own, the chatbot needs context from the prior conversation. The previous query was about cycling distance, so “what about last year?” could mean the user wants to know the previous year’s total (2023) or some other comparison. However, the bot already pre-emptively mentioned 2023 in the last answer. To be sure, or consider a different context, the bot will clarify.  
2. **Chatbot Clarifies:** The chatbot responds: *“Do you mean how far you cycled in the year before (2023)?”* It chooses this interpretation because that’s the most logical follow-up given the context. (If multiple datasets were in play, it might ask which data or which metric the user is referring to.)  
3. **User Confirms:** The user replies, *“Yes, compare 2024 to 2023.”* Now the intent is clear – the user wants a comparison between years.  
4. **System’s Data Retrieval:** The system runs a query (or two) to get total cycling km for 2023 (from the other table) as well as 2024 (which it already has, possibly cached from earlier).  
5. **Chatbot’s Answer:** The chatbot now provides a comparative answer: *“In 2024 you cycled **3,560 km**, whereas in 2023 you cycled **3,421 km**. That’s an increase of about 4%.”* It might also present this in a more visual form if available – for example, a small bar chart icon could be offered that, when clicked, shows a bar chart of 2023 vs 2024 distance. (Such a feature might come in a later version, not MVP.)  
6. **User Feedback:** The user appreciates the clarification step ensuring they were asking the intended question. This dialogue demonstrates the chatbot’s ability to handle pronouns and time references in follow-ups by leveraging conversation history.

*Storyboard Note:* This flow would be depicted as a short back-and-forth exchange in the chat interface. It highlights the chatbot’s polite clarifying question and the maintenance of context between turns. The design principle is to never leave the user with an unanswered or wrongly answered question when a simple clarification can resolve ambiguity.

### 4. Complex Query (Joining Multiple Datasets)  
**Scenario:** The user asks a more complex question that involves correlating two different datasets.  
1. **User’s Question:** Now the user asks: *“Did I tend to run longer distances on days when I slept better?”* This question involves two distinct datasets: perhaps a *Running Activities* dataset (distance per run, date) and a *Sleep* dataset (hours of sleep per night, date). The user has uploaded both: one from a fitness app and one from a sleep tracking app.  
2. **Chatbot Analysis:** The LLM parses this and identifies two data needs: running performance and sleep quality, and that it’s asking for a correlation or comparison (“on days when I slept better” suggests filtering by sleep data). It realises it should look at days as the common key. It checks that the user indeed has those datasets available (if not, it would say it lacks the data). In this case, both exist. “Slept better” likely implies above-average sleep or some threshold; the bot might interpret it as the top quartile of sleep hours or when sleep > X hours. It decides on a strategy: find the average running distance on *good sleep* days vs *bad sleep* days.  
3. **System’s Data Retrieval:** The orchestrator executes a join in BigQuery: joining the Running table and Sleep table on date. It then computes, for example, the user’s median or average run distance on days with >=8 hours of sleep vs <8 hours of sleep (assuming 8h as a threshold for “good sleep”). The results might show: on good sleep days, average run = 8.5 km; on poor sleep days, average run = 6.7 km.  
4. **Chatbot’s Answer:** The chatbot responds with an insightful answer: *“It appears so. On nights when you slept 8+ hours, your runs next day averaged about 8.5 km. On shorter-sleep nights, they averaged 6.7 km. In general, better sleep aligned with longer runs.”* The answer merges data from both sources and delivers a conclusion. It may couch the conclusion carefully (since correlation is not causation, a thoughtful tone like *“This suggests a correlation: you ran further on well-rested days.”*). The style is analytical yet accessible, much like *The Economist* explaining a personal trend with data.  
5. **User Reaction:** The user finds this multi-dataset answer very powerful – it’s the kind of insight that traditionally would require manual data wrangling. They might follow up with “What about pace?” or some related question, and the cycle continues.

*Mockup Illustration:* This could be shown with a combined data chart or at least the textual answer clearly referencing both sleep and run data. Perhaps an icon or tag on the answer indicates that it used multiple datasets. For example, little icons or labels “Run🏃 + Sleep💤” could appear next to the answer to transparently show which data sources were used (this is a possible UI enhancement to build user trust in where the answer comes from).

### 5. Handling Missing Data Gracefully  
**Scenario:** The user asks something for which data is not available.  
1. **User’s Question:** Suppose the user asks: *“How many calories did I consume last week?”* However, the user has not uploaded any diet or calorie tracking data to the system.  
2. **Chatbot Response:** The chatbot’s planner searches the metadata of available datasets and finds nothing related to nutrition or calorie intake (as opposed to calories burned, which might be in fitness data but the wording “consume” indicates food). Recognising this, the chatbot responds apologetically and helpfully: *“I’m sorry, I don’t have your dietary data to answer that. If you track your meals or calories, you can upload that data and I’ll be able to assist.”* It may guide the user to add such a dataset by providing a direct link or instructions.  
3. **User Outcome:** The user now knows the limitation. Maybe they indeed have a food log CSV somewhere – they proceed to upload it. Once uploaded, the user can re-ask the question and now get an answer. The chatbot’s graceful handling of missing data ensures it doesn’t try to guess or provide a wrong answer. This maintains trust; as noted in other RAG-based chatbots, explicitly saying “cannot answer” when data is lacking helps reduce misinformation.  
4. **System Adaptation:** Internally, when the user adds the new dataset (say a calorie log), the system will include it in future query planning. The next time the question is asked, the chatbot might combine the calorie data with activity data (if relevant) or answer directly from the new dataset.

### 6. Ongoing Updates and Continuous Learning  
**Scenario:** The user continues to use the chatbot over time and adds more data; the system improves in understanding the user.  
- Over weeks, the user might add monthly updates to their datasets (the architecture supports appending new records, which are immediately available for querying). The chatbot could proactively acknowledge new data, e.g., *“I see you’ve added your August fitness data. You can ask me about your summer fitness summary now.”* (Such proactive tips would be a later feature to engage users.)  
- The chatbot also refines its prompts or employs fine-tuning as it gathers more interactions. For instance, if the user often asks about “last month” vs “this month”, the system might start offering a default comparison in answers.  
- Throughout, the interface remains the chat window as the primary mode of interaction, possibly supplemented by a side panel showing what data sources are currently in use or when they were last updated.

These interaction flows demonstrate the user-centric design of the chatbot. The **storyboards** highlight intuitive conversations: uploading data, asking questions freely, and receiving insightful, context-aware answers. In all cases, the complexity (data joins, disambiguation, query logic) is handled behind the scenes by the architecture described earlier. Next, we formalise the product definition, including user personas, key use cases, features, and the security model, to solidify the requirements and scope.

## Product Definition and Scope  

### User Personas and Use Cases  
We envision several archetypal users (personas) for this personal data chatbot, each with distinct goals. These personas inform the features and priorities of the product:

- **The Quantified Self Enthusiast (Adam):** Adam is a tech-savvy individual who tracks everything – fitness, sleep, diet, location, and more. He wants a single interface to query *all* his personal stats and find patterns. Use cases: *“Compare my average sleep on weekdays vs weekends,” “Which month did I walk the most steps?”* Adam values depth of analysis and will use advanced features like multi-dataset queries and visualisations. He pushes the system’s ability to handle lots of data types together.

- **The Fitness Fanatic (Bella):** Bella mostly cares about her workout data (runs, cycles, gym sessions) and health metrics (heart rate, calories burned). She uses devices and apps that export data which she uploads regularly. Use cases: *“How has my running pace improved over the last 6 months?”, “Did I burn more calories on days I do strength training or cardio?”* She may not be extremely technical, so she relies on natural language questions. Consistency and accuracy of answers are key for her, as she might base training decisions on them.

- **The Busy Professional (Charlie):** Charlie is interested in productivity and communication patterns. He uploads his work calendar data, chat logs (e.g. Slack or email metadata), and perhaps time-tracking data. Use cases: *“How many meetings did I have last quarter compared to the previous one?”, “Who are the top five people I emailed most this month?”* Charlie uses the chatbot to reflect on how he spends his time and with whom. He values quick, factual answers and might export some results to share or present at work (e.g. how his client contacts grew).

- **The Music Aficionado (Dana):** Dana connects her Spotify listening history and perhaps mood journal entries. She’s curious about how her music listening correlates with mood or activity. Use cases: *“What genre did I listen to most during my workouts?”, “Was I happier on days when I listened to jazz?”* Dana’s use pushes the system into combining media history with either fitness or journal data, requiring both structured and unstructured analysis (e.g. sentiment from journal text). She values the novelty of questions that haven’t been pre-baked in any app.

- **The Privacy-Conscious User (Evelyn):** Evelyn is generally like one of the above personas but is extremely cautious about her data. She will scrutinise the security model and needs assurance that her information is safe. Use case: She might test the system with smaller or obfuscated data first, and ask meta-questions like “How do you protect my data?” (which the chatbot could answer from a help documentation dataset). Evelyn’s concerns drive the emphasis on transparency and trust in features.

These personas highlight broad categories: **health and fitness tracking, personal productivity, media consumption, social interactions**, and an overall **data nerd**. The product’s feature set must be flexible to accommodate all, without assuming everyone will use every feature.

### Key Features and Functional Scope  
From the personas and problem statement, we define the core features of the chatbot application:

- **Natural Language Question Answering:** At its heart, the chatbot allows users to ask questions in everyday language about their data. This includes:
  - Temporal queries: understanding phrases like “last year”, “the past week”, “between March and June”, and translating them to date filters accurately.
  - Comparative queries: e.g. “compare A and B” or “which is higher, X or Y?” leading to multi-part answers or even simple charts.
  - Aggregation and summary: answering “how many/much”, “average of…”, “maximum/minimum” etc., by translating to the appropriate SQL aggregations.
  - Detail queries: e.g. “list all events where…” – the chatbot can also retrieve records or examples (though returning very large lists is discouraged; the bot might summarise or show top N items by default).

- **Multi-Dataset Reasoning:** The ability to draw insights from multiple datasets at once. This includes joining on common keys (like date, location) or even without explicit keys (using time overlap or semantic similarity). For example, combining heart rate and music tempo requires aligning by timestamp; the system handles such logic if the question implies it. This feature turns the chatbot into a personal data analyst that can find relationships (correlations, sequences) across different facets of one’s life.

- **Contextual Conversation & Follow-ups:** Users can have a conversation with the bot. It remembers context within a session, so users can ask follow-up questions without restating all parameters. The bot can resolve pronouns or ellipsis (e.g. “and what about July?” following an earlier query about June). We scope this to session-based context (when the user returns later or after a long gap, the context might reset for privacy and simplicity, unless we choose to implement long-term memory explicitly). This feature requires careful handling to avoid confusion, and the bot will re-clarify if a follow-up is ambiguous out of context.

- **Clarification and Guidance:** The chatbot will actively ask clarifying questions if it’s unsure what the user means. Rather than guess and risk a wrong answer, it uses a cooperative approach: *“I have data on both running and cycling; which one did you want to ask about?”* or *“By ‘best’, do you mean highest value or something else?”* This interactive disambiguation is a feature that ensures accuracy and user satisfaction. It essentially implements a dialog management atop the raw LLM responses.

- **Data Source Management:** A user-friendly interface for managing personal datasets:
  - Uploading new datasets (through UI or potentially via API integrations for advanced users).
  - Viewing what data is uploaded: a list of datasets with metadata like last updated, number of records, and recognized fields.
  - Updating or deleting datasets: e.g. remove a dataset or replace it if needed.
  - Perhaps basic preview or stats (like a small summary: “Cycling: 1,045 entries from Jan–Dec 2024”).
  - Tagging/categorising datasets so the system knows what kind of data it is (the user might help by selecting a category on upload as we saw).

- **Extensibility for Data Schema:** The system should handle slightly varying schemas by configuration rather than code. This means features like a *schema mapping assistant* during upload could be included: if a user’s file columns don’t match known patterns, allow them to specify what each column represents (e.g. ask “Which column is the timestamp?” if not obvious). This way the user actively teaches the system, which stores that mapping. This feature ensures broad compatibility with whatever data the user has, as long as it’s structured.

- **Result Presentation:** While text answer is the primary mode (and required for MVP), the product should eventually support rich answers:
  - **Visualisations:** If a user asks for something trend-like or comparative, the bot can generate a simple line chart, bar chart, or pie chart as appropriate. Initially, it might just provide textual descriptions (“in a chart, you’d see X rising”), but an advanced feature would be an auto-generated chart image. GCP’s ecosystem or libraries (like Matplotlib, Altair, or Google Charts via an API) could be used to create charts on the fly. For GA, we envision the chatbot can show, for instance, a line graph of step count over the week when asked “Show me a graph of my steps this week”.
  - **Tables or Lists:** For queries like “List all days I exceeded 10,000 steps,” the bot might present a brief table of dates and step counts, or a bulleted list in the chat. There will be sensible limits (e.g. show top 5 results and then offer “and 12 more… export?”).
  - The style of presentation will follow a clean, minimalist aesthetic (in line with *The Economist* style – informative but not flashy). 

- **Personalisation and Learning:** Over time, the chatbot could personalise its behavior:
  - Remember user preferences (if a user always prefers distance in miles instead of km, the bot could learn that).
  - Allow the user to set certain defaults (preferred units, or linking datasets – e.g. mark certain datasets as “vacation periods” to be used when that concept is invoked).
  - Possibly allow custom aliases for datasets (if user says “my Garmin data” it knows which dataset that refers to).

- **Security & Privacy Features:** (Detailed later, but as part of product scope, features include account security, data encryption, compliance options like data export and deletion by user command, etc.) For example, an **Account Settings** page lets the user download all their data or permanently delete their account and data (to comply with right-to-erasure laws).

- **Help and Onboarding:** Built-in guidance such as example questions, tutorials on how to phrase queries, and perhaps a “recipe book” of interesting questions for each data type. This is a softer feature but important for user adoption – some may not immediately know what to ask. We might provide templates like “Try asking: ‘How did my average heart rate change last month?’”.

The **feature scope** will be phased (not everything in MVP). The MVP focuses on core Q&A and data upload; visualisations and heavy personalisation come later, for example. Importantly, the chatbot will *not* do a few things: it won’t allow querying other users’ data (no social features in v1, to avoid privacy issues), it’s not a general web search or knowledge chatbot about the world (it specifically answers about the user’s data, though it has general knowledge to interpret questions), and it’s not initially providing real-time alerts or push notifications (it’s user-driven Q&A, though that could be a future direction – e.g. “tell me each week how I did”).

### Constraints and Assumptions  
In developing this product, we recognise certain constraints and make assumptions:

- **Data Volume:** Each user’s data is assumed to be moderate in size (1K to 100K rows per dataset, a few megabytes to tens of MBs). The architecture is capable of more, but our design and costs assume personal-scale data, not enterprise-scale. If a user tries to upload millions of rows (say their entire tweet history), BigQuery can handle it, but UI and query performance might degrade without adjustments. We assume typical users have on the order of dozens of datasets, not hundreds.

- **Data Schema Consistency:** We assume that within a broad category (e.g. workouts, messages, location logs), most users’ data will share common traits (dates, identifiers, metrics). The system is designed to handle variations, but if a dataset is completely unstructured or very unusual, the LLM might need additional hints. We assume users will use the guidance provided to map their data correctly. Edge cases where data is very sparse or oddly formatted might require them to preprocess it (we won’t initially attempt to magically clean or reshape badly structured data).

- **Real-Time Updates:** The system isn’t initially intended for streaming real-time data or continuous queries. It assumes data is ingested in batches (like daily or ad-hoc uploads). Questions about “today so far” are answerable if the user just uploaded today’s data or if we integrate with an API to fetch the latest. But the MVP will not include streaming ingestion (that could be in a later phase with something like Pub/Sub feeding BigQuery). So a constraint is that data might be as fresh as the last upload; we will communicate to users if their data is old or if a query’s time range goes beyond what’s available.

- **LLM Limitations:** The reliance on LLMs means occasional errors in understanding or responding. We mitigate this with system prompts and few-shot examples to improve reliability. However, we assume the chosen Vertex AI model is sufficiently advanced (by 2025, models like Gemini are state-of-the-art) to handle most queries well. There is a constraint of **cost and latency**: calling a large LLM for every single message can be expensive and slow. To manage this, we might use a smaller model or heuristic for initial parsing and only use the big model for final answer composition. We assume a budget that allows each user to make a reasonable number of queries per month without exorbitant cost (perhaps using caching and efficient query techniques to minimise LLM token usage). The design must be mindful of this constraint, using BigQuery to do heavy lifting (since SQL operations are cheaper than large LLM context lengths, for example).

- **Privacy and Trust:** We assume users will only join if they trust the platform. Thus we have a near-zero tolerance for data leaks or cross-user access. We also expect users to provide truthful data (the system isn’t validating the accuracy of what they upload, it just uses it). If a user uploads nonsense, the answers will be nonsense relative to that data; that is acceptable since it’s their own data. We focus on not outputting anything beyond the data scope. Another assumption: users are okay with their data being processed by Google Cloud services (this should be clear in terms – e.g. that their data will reside in BigQuery on GCP). Highly sensitive users might ask for on-prem or self-hosted versions, but that’s out of scope for now (our SaaS offering is in the cloud).

- **Multi-tenancy Implementation:** We choose a particular multi-tenant approach (logical isolation in BigQuery with user-specific identifiers). The assumption is that this will be sufficient for our user base and simpler to manage than thousands of projects. If we were to sign up enterprise clients or have stricter isolation needs, we might revisit that. But for a SaaS for individuals, the overhead of separate projects per user is high, so we assume a single project with careful dataset isolation works (with guardrails in code and IAM). We will thoroughly test that one user cannot ever access another’s data via any path.

- **Compliance:** We assume from the start compliance with major regulations (GDPR for EU, CCPA for California, etc.) will be necessary once the product scales. As such, data export/delete and consent mechanisms are planned. We also note that since this is personal data provided by the user themselves, in many regimes the user giving their own data to a tool for their own use is less problematic than a company collecting user data unbeknownst to them. Nevertheless, our policies and features will align with privacy best practices.

### Security Model  
Security is a first-class concern in this application, given the personal and potentially sensitive nature of user data. The security model spans authentication, authorisation, data handling, and platform security:

- **Authentication:** All users must authenticate to access the service. Likely we will implement OAuth 2.0 login with trusted providers (Google, etc.) or our own email/password with multi-factor. Sessions will be securely managed (HTTP-only cookies or token-based auth to the frontend, and tokens passed to backend API calls). The chat interface and data management UI are only accessible after login. We will use **Cloud Identity Platform** or Firebase Auth for a robust and secure identity solution, unless we integrate with existing accounts.

- **Authorisation & Access Control:** After auth, every request on behalf of a user carries their identity context. The backend checks this against the target resources. For data queries, the mapping of user -> BigQuery dataset is strictly enforced. For example, if user 42 is logged in, the backend will only query tables in dataset `user42_*`. Even if a malicious user tried to craft a request for another dataset, the backend will validate the dataset belongs to them. Additionally, at the BigQuery level, we can implement **view-based access control** or separate service accounts: for instance, one approach is to have a Cloud Run instance per user with a specific service account, but that’s not scalable for many users. Instead, we rely on our application logic plus careful IAM: The service account used by the app has permissions to query all user datasets, but our code never allows cross-user queries. We will also implement monitoring/alerting for any anomalous access patterns in BigQuery (like if someone somehow tried to union all datasets). Data in Cloud Storage (uploads) similarly will be prefixed or segregated by user and only accessible via backend after auth.

- **Encryption and Data Protection:** As mentioned, all data at rest in BigQuery is encrypted by Google-managed encryption keys by default. We have the option to use **Customer-Managed Encryption Keys (CMEK)** for BigQuery and GCS if enterprise users require it. In transit, all communication uses TLS. Internally between services (Cloud Run to BigQuery etc.), it’s within Google’s network and/or also TLS protected. If we store any sensitive config (like API keys if users link external services), those are kept in **Secret Manager** and not in plain text.

- **LLM Data Handling:** When we send data to Vertex AI for the LLM to process, we also consider that a form of data handling. Vertex AI’s terms ensure that data is not used to train the model or shared, but we might still minimise sending raw personal identifiers. For example, if the data contains names of people the user messaged, and the user asks “Who emailed me the most?”, the answer requires a name. But we might instruct the LLM not to reveal names from the data unless explicitly asked, to avoid any accidental leakage beyond answering the question. All prompt assembly is done backend side, so the user doesn’t see raw data unless it’s part of the answer.

- **Audit and Logging:** Every query executed and data access can be logged (without logging the actual personal content). We will keep logs like “User X ran query on dataset Y at time Z” and high-level metrics (e.g. count of records returned). This is useful for audit if something goes wrong and for the user’s own transparency (we could even show them their recent queries history, which is both a UX feature and a security feature). GCP’s Cloud Audit logs will also automatically record BigQuery job details and Cloud Storage access, which helps in any security review.

- **Multi-Tenant Testing:** We will extensively test with dummy users that there is no data bleed. For instance, create two users with similar questions and ensure each only gets answers from their dataset. We may also incorporate **fuzz testing** for the query planner to ensure a prompt from user A can never include content from user B because the planner logic simply has no knowledge of other users’ data (each user’s metadata registry is separate or keyed by user).

- **Session and CSRF Protection:** In the web UI, normal web security practices apply. We will protect against cross-site request forgery by using proper tokens and not exposing unsafe endpoints. The API will not allow requests without auth. We’ll also have rate limiting to prevent abuse (someone spamming thousands of queries, or denial-of-service attempts). Each user might be limited to a reasonable number of queries per minute – this doubles as cost control.

- **Backups and Data Retention:** User data being valuable, we will backup datasets periodically (perhaps via scheduled BigQuery exports or using point-in-time recovery features). But we also allow users to truly delete data if they want, which means backups must eventually purge that data too (respecting retention policies). The user will be informed about how long a deleted dataset might linger in backups (perhaps 30 days) unless an immediate hard delete is requested.

- **Compliance and User Control:** The security model aligns with privacy laws: users can download their data (data portability) and delete it (right to delete). We will have a privacy policy clearly stating that the data is used only to provide answers to the user themselves, and not for any advertising or selling. Because this is a multi-tenant SaaS, we, the provider, do have access to the infrastructure – but we will implement administrative access controls such that no staff can arbitrarily query user data without approval (and if support needs to help a user with an issue, it’s done in a controlled way with their permission). Possibly, we can even encrypt each user’s data with a key derived from their password (end-to-end style) for ultimate privacy, but that complicates the ability to use LLM on it. For now, we trust GCP’s robust internal security.

In short, the product is designed with a **zero-trust mindset**: assume no user data should be accessible unless explicitly authorised. GCP’s built-in security features (IAM, audit logs, encryption) provide a strong foundation, and we build on that with careful application-layer checks. 

Now that we have described what the product will do and how it will do it securely, we turn to the development roadmap. This will outline how we plan to build and roll out the product in phases, from a minimum viable product to a full general availability release.

## Development Roadmap (MVP to GA)  
Building this multi-faceted product will be an iterative journey. We propose a phased roadmap with clear milestones, ensuring that at each stage we deliver a working product, gather feedback, and then expand capabilities. The phases include a Minimum Viable Product (MVP), a Private Beta, and a General Availability (GA) launch, with potential intermediate milestones as needed.

### Phase 1: Minimum Viable Product (MVP)  
**Goal:** Deliver a functional core product to a small set of pilot users, focusing on the essential use case – asking questions about one’s data and getting correct answers.  

**Scope of MVP Features:**  
- **Data Upload:** Allow users to upload at least one dataset (CSV). Keep the schema handling simple (perhaps require a template or provide a few presets for common data types like a sample “steps.csv” format). 
- **Basic Q&A:** Support natural language questions on a single dataset. The MVP might handle one dataset at a time (the user may have to select which dataset they want to query, or the question must explicitly name it) to reduce ambiguity.
- **LLM Integration:** Use Vertex AI’s text model for parsing and answering, but in MVP we can start with simpler prompts and perhaps a smaller model (to keep cost down while testing). Possibly we restrict question complexity initially.
- **UI:** A basic web interface: login page, data upload page, and a simple chat interface. It need not be beautifully styled yet, but clear enough. No rich charts, just text answers. Possibly even a one-turn query interface (ask question -> get answer, not a full chat memory) to simplify.
- **Multi-Tenancy Baseline:** The architecture is set up for multi-tenancy (even if only a handful of users in pilot). Ensure isolation works. Possibly limit MVP to, say, 10 pilot users to monitor system behavior.
- **Security:** Basic auth, using a quick solution (e.g. Firebase Auth can accelerate this). Ensure each user’s data is separate as planned. For MVP, we might not implement user-managed encryption keys, but rely on GCP defaults.
- **No Complex Joins:** We might postpone multi-dataset queries. MVP will focus on questions answerable from single tables. This reduces complexity in query planning. If a user has multiple datasets, they can query them separately for now.
- **No Clarification Dialogues (Reactive only):** If a question is ambiguous, the MVP bot might either take a best guess or return a generic “I’m not sure how to answer that” rather than engage in multi-turn clarification. Interactive clarification can be planned but may not make the first cut.
- **Deployment:** Deploy on GCP (maybe all in one project). Use Cloud Run for backend, BigQuery for data. We keep manual oversight on the system. Possibly not worrying about full auto-scaling since user count is small, but the components inherently scale if needed.

**Timeline & Deliverables for MVP:** We estimate ~3 months for MVP development given a small agile team. Key deliverables:
- Basic UI/UX design (wireframes) and implementation.
- Back-end services: file upload handling, BigQuery integration, calling Vertex AI API.
- A set of **unit tests** for the query logic (e.g. given a known dataset and a question, does it produce the right SQL and answer).
- Internal demo with the pilot users’ own data to gather feedback.
- Documentation for MVP: quickstart guide for users, and an architecture doc (maybe this document) for internal reference.

**Success Criteria for MVP:**  
   - Users can successfully upload a dataset and get at least 5-10 different questions answered correctly.
   - System correctly interprets basic time phrases (day, week, month names, last X).
   - No major... major errors in accuracy or security observed during the pilot. Users should report that the system answers questions correctly (at least for straightforward queries) and that they feel confident uploading their data.  

### Phase 2: Private Beta  
**Goal:** Expand capabilities and test with a broader audience (e.g. a closed beta with dozens of users). Incorporate feedback from MVP and add important features that were deferred.  

**Scope of Beta Enhancements:**  
- **Multiple Datasets & Joins:** Enable the chatbot to handle questions that involve more than one dataset. This includes implementing the more advanced query planner and testing common multi-source queries (like correlating sleep and exercise). The LLM prompts are refined to manage this complexity.  
- **Clarification Dialogues:** Introduce the interactive clarification feature. The bot will now ask follow-up questions when needed, rather than giving up or guessing. This requires building a simple dialogue manager to track pending clarifications.  
- **Semantic Search Integration:** If users have textual data (messages, notes), implement the vector embedding and similarity search pipeline. Likely using BigQuery’s ML capabilities to generate embeddings and search within them. Beta users can then ask questions like “When did I mention project X in my chats?” and get relevant snippets.  
- **UI Improvements:** Refine the chat UI with quality-of-life features: loading spinners while the bot thinks, the ability to scroll through past Q&A, perhaps a button to copy an answer or export data. Add a basic **visualisation** component – maybe start with static charts for a couple of query types (like time series line chart for trend questions). The UI should also present the list of data sources more clearly and allow switching context if needed (e.g. a dropdown to focus the bot on a particular dataset, if that makes interaction easier).  
- **Data Management:** Beta might introduce integration connectors (for example, import from Google Fit or Strava via API, not just file upload). Even if limited, demonstrating one live integration will set the stage for future extensibility.  
- **Security & Privacy:** By beta, implement full user account controls: password resets, email verification, etc., and allow users to delete datasets on their own. Conduct a security audit or pentest with a small group to ensure no vulnerabilities. If not already done, enforce stricter IAM (maybe moving toward the per-user service accounts model if feasible). Beta is also when we draft clear privacy policy and terms of service documents, as more users will be involved.  
- **Scalability Testing:** With more users, test the system under heavier load. We will simulate concurrent queries and larger data to ensure the app scales. Optimize where needed (e.g. adjust BigQuery slots, add caching of embedding results, etc.). Monitoring dashboards on GCP (Cloud Monitoring) will be set up to track latency of answers and error rates.  

**Beta Timeline & Activities:** Another ~3-4 months post-MVP. We’ll onboard, say, 50-100 users gradually, perhaps by invitation. We’ll use their feedback to find out which features are most used and where the bot fails. This phase is also about **UX refinement** – simplifying any workflows that confused users in MVP (for instance, if schema mapping was too technical, we make it smarter or more guided).  

**Success Criteria for Beta:**  
   - The chatbot can handle at least 80% of user questions without developer intervention or crashes. Multi-dataset queries succeed with correct results in test cases.
   - Users report the clarifications make the bot feel smarter and more conversational.
   - Average answer latency remains acceptable (e.g. under 5 seconds for typical questions).
   - No data leaks or privacy incidents with the larger user pool. Trust indicators (like users uploading more data over time) should increase.
   - We have enough feedback to decide what is needed for GA in terms of features or fixes.

### Phase 3: General Availability (GA) Launch  
**Goal:** Open the service to all (public launch), with a polished, scalable product. At GA, the focus is on robustness, compliance, and broad usability.  

**Scope of GA Features and Improvements:**  
- **Full Feature Set:** All major promised features are in place: seamless multi-data querying, clarifications, vector search, and a user-friendly interface. Visual answers (charts) are now supported for a range of query types, enhancing the appeal of the answers. The chatbot’s tone and style have been refined to be helpful and in line with the brand voice (professional yet friendly).  
- **Performance Optimisation:** Fine-tune the system for speed and cost. Possibly implement a caching layer for frequent queries (if many users ask similar questions, or if a user repeats a query). Ensure the LLM usage is efficient – e.g., use streaming responses so the user can see answers as they are being formulated, or use smaller models for certain tasks without sacrificing quality. BigQuery costs will be monitored; we might introduce some query limits or smart scheduling (like heavy analysis can be done asynchronously if the user asks for something like year-long analysis with millions of data points).  
- **Scalability:** By GA, the system should support thousands of users. This might involve moving to multi-region deployment if user base is global, to reduce latency (using Cloud CDN for static content, and perhaps deploying backend in multiple regions). BigQuery can handle multi-terabyte scale, so that’s fine; we just need to ensure our usage of it is within quotas or we have reservations as needed. Auto-scaling rules for Cloud Run are well-tested to handle traffic spikes (like when we get a burst of new users on launch day).  
- **Enterprise-Readiness:** Even though product is aimed at individuals, GA might attract interest from small teams or organisations. We might add features like team accounts or data sharing as future work. At GA, we ensure compliance (SOC2, GDPR, etc. as needed) so that even privacy-conscious users (like Evelyn persona) are satisfied. This could include a final external security assessment.  
- **Documentation and Support:** Provide comprehensive user documentation, a knowledge base of example queries, and support channels. Perhaps integrate a feedback mechanism in the chat (a thumbs up/down after an answer, to learn which answers are good or where the bot misunderstood – this can help continuously improve the model or prompt).  
- **Pricing Model Implementation:** Decide if the product will have a free tier and paid tiers (since it’s SaaS). By GA, integrate billing if needed (for example, free for up to X queries or data size, then subscription). This affects the technical side minimally, but we need usage tracking to enforce limits.  

**Post-GA and Future:** After GA, regular updates will add new connectors (plugging into more APIs so users don’t always have to upload manually), possibly a mobile app for on-the-go querying, and exploring proactive analytics (the bot suggesting insights unprompted). Another future direction could be allowing the user to ask questions like “How do I improve?” and the bot can give advice based on data (venturing slightly into coaching). But these are beyond the initial GA scope and would be planned based on user demand.

The roadmap above ensures we deliver value early (MVP), validate it with real users (Beta), and then launch a stable, feature-rich product (GA). Each phase builds upon the last, guided by user feedback and technical feasibility. Throughout all phases, we remain aligned with the core vision: empowering users to interact with their own data as easily as having a conversation.

## Example Prompts and Acceptance Criteria  
Finally, to tie everything together and ensure each feature works as intended, we present example user prompts along with the expected system behaviour and criteria for acceptance. These examples serve both as **user-facing scenarios** and as **test cases** for development. They illustrate how natural language inputs should be handled by the system.

- **Temporal Query Understanding:**  
  - *User Prompt:* “How many messages did I send last year?”  
  - *Expected Behaviour:* The system interprets “last year” in context (if today is 4 April 2025, then “last year” refers to the year 2024, i.e. 1 Jan 2024 – 31 Dec 2024). It identifies the *Messages* dataset and counts messages in that date range.  
  - *Acceptance Criteria:* The chatbot returns a correct count of messages for 2024. It perhaps says, “You sent 5,432 messages in 2024.” The term “last year” is correctly resolved; if the current date changes, the interpretation should adjust accordingly. A test would be to verify that on Jan 1, 2026, asking the same question yields the 2025 count, not 2024. The answer should come with no prompting from the user about how to filter by date – it’s automatic.

- **Aggregation and Comparison:**  
  - *User Prompt:* “Did I run more in the first half of this year or the second half?”  
  - *Expected Behaviour:* The chatbot breaks this down: “first half of this year” = Jan–Jun 2025, “second half” = Jul–Dec 2025. It sums running distance (assuming a Running dataset) for each period and compares them.  
  - *Acceptance Criteria:* The answer explicitly compares the two values, e.g. “You ran 220 km in Jan–Jun 2025 and 180 km in Jul–Dec 2025, so you covered more distance in the first half.” The bot should handle the concept of “this year” as the current year and correctly split halves. We test edge cases like asking in July (one half is past, one half is future partial) – the bot should clarify if needed or say second half is incomplete so far. The acceptance is that it doesn’t confuse the time ranges and the math is correct as per the data.

- **Multi-Dataset Join Query:**  
  - *User Prompt:* “On nights when I slept at least 8 hours, what was my average heart rate the next day?”  
  - *Expected Behaviour:* The system uses Sleep data and Heart Rate data. It filters sleep records for hours >= 8, takes those dates, then looks up heart rate (perhaps daily average HR) for the next day’s date. It then aggregates those heart rates (average of averages, effectively).  
  - *Acceptance Criteria:* The chatbot might answer, “On days following a good night’s sleep (≥8h), your average heart rate was 60 bpm.” The key criteria: it correctly links the two datasets by date relationship (next day), and does the calculation. We would acceptance-test this by manually computing a known sample and ensuring the bot’s answer matches. If the user had no overlapping data (e.g. no nights of ≥8h), the bot should say it doesn’t have sufficient data for that query. If only partial heart rate data exists, it should handle gracefully or note it. The user didn’t explicitly mention datasets, but the bot figured out to use Sleep and Heart Rate data – that’s an important acceptance point for the intent recognition.

- **Clarification Dialogue (Ambiguity):**  
  - *User Prompt:* “Tell me about last week.” (Assume this is the conversation opener, with no prior context.)  
  - *Expected Behaviour:* The request is vague – last week’s what? The chatbot should not guess blindly. It should respond with a clarification question, such as: “Sure, I can summarise your last week. Is there something specific you’d like to know (e.g. your steps, messages, or any particular metric)?”  
  - *Acceptance Criteria:* The bot asks a clarifying question that mentions available categories of data, rather than saying “I don’t understand.” We then expect the user’s next input to be something like “My fitness activities” or “Everything,” and the bot would then proceed. We test that the clarification logic triggers for open-ended prompts. If the user instead had asked “How was my sleep last week?”, no clarification is needed because it’s specific – the bot should directly answer. So acceptance is that ambiguity triggers one follow-up question from the bot and that subsequent answer after clarification is correct.

- **Missing Data Handling:**  
  - *User Prompt:* “How many calories did I consume last month?” (User has not uploaded any diet or calorie intake data.)  
  - *Expected Behaviour:* The system finds no dataset relevant to “calories consumed” (it might have “calories burned” in workouts but that’s different, and it likely recognises the intent is food intake). It should inform the user it doesn’t have that data.  
  - *Acceptance Criteria:* The chatbot responds with a polite refusal and a helpful suggestion: e.g. “I’m sorry, I don’t have your dietary intake data to answer that. If you track your meals or calories, you can upload that dataset and ask again.” This message must contain no fabricated number. We accept the feature if the bot correctly detects the absence of required data and produces a helpful response, rather than a misleading or confusing one. We’d test by asking various questions for which we intentionally did not upload data (like ask a fitness question without fitness data, etc.) – the bot should consistently respond that it lacks data.

- **Personal Data Privacy (Negative Test):**  
  - *User Prompt:* (Malicious user attempt) “Show me Alice’s step count for today.” (Where Alice is another user on the platform.)  
  - *Expected Behaviour:* The system should treat this as an unauthorized query. Since the chatbot is only aware of the current user’s data, it would likely respond as if asked about an unknown entity: “I’m sorry, I’m not sure who Alice is in this context.” It should **not** reveal anything about another user’s data (and indeed it technically can’t, because it has no access).  
  - *Acceptance Criteria:* Even if a user tries to frame a question to get someone else’s data, the system never returns it. In internal testing, we might simulate multi-user environment where one user’s context is active and ensure queries always stay constrained. The acceptance is more a security test: that cross-tenant data access is impossible through the chatbot. This is reinforced by design (each user only has their own data in scope), but we include it to highlight the importance of this criterion.

- **Follow-up Context Carryover:**  
  - *User Prompts:*  
    1. “What was my average running speed last week?”  
    2. (after receiving answer) “How about the week before that?”  
  - *Expected Behaviour:* For the first question, the bot calculates average running speed (distance/time) for last week (e.g. 8–14 days ago). For the second question, phrased as a follow-up, the bot should understand it refers to the same metric (running speed) but the previous week. It then produces the answer for that week. Possibly it also compares or at least stays in context that we are talking about running speed.  
  - *Acceptance Criteria:* The second answer should clearly be about the week prior and not ask “the week before what?”. It might say “The week of March 18–24, your average speed was ...”. If the user’s follow-up was even shorter like “And the week before?”, it should still get it from context. We accept this feature if in testing the chatbot can carry context for at least one or two turns reliably, and it references the prior query subject correctly. If the context is lost, that’s a bug to fix. We also ensure that context doesn’t bleed incorrectly (if the user changes subject, the bot shouldn’t confuse with old context).

- **Prompt/Response Quality (LLM reasoning):**  
  - *User Prompt:* “Find any trends in my music listening versus mood.”  
  - *Expected Behaviour:* This is a complex request that might require the bot to do some reasoning (maybe correlating two datasets in a qualitative way). It might retrieve data (music play counts, mood ratings) and then use the LLM to find a trend (e.g. “you tend to listen to upbeat music on days you report high mood”).  
  - *Acceptance Criteria:* The answer should be fact-based and reasonable, not a hallucination. If no strong trend exists, the bot should say so (“I didn’t find a clear trend between your music and recorded mood”). The acceptance is that the LLM’s generative part stays grounded in data – which we ensure by always providing it data context (the RAG approach). We would test a scenario with known correlation and see that the bot finds it, and test a scenario with no correlation to see that it doesn’t invent one. The quality of language should also be high: clear, concise, and in correct British English. For instance, using phrasing like “whilst” or “amongst” appropriately, and proper spelling (e.g. “organise” not “organize”), matching our style guideline.

Each of the above examples doubles as an **acceptance test**. For development, we will likely encode many of these as automated tests (where we simulate the user input and verify the output meets certain criteria). We will also use them as **agile story acceptance criteria**: e.g. for the story “As a user, I want to compare two time ranges,” the acceptance is the scenario with first half vs second half of year as demonstrated. 

By adhering to these criteria during implementation, we ensure the final product meets the high expectations for accuracy, usability, and security. This approach – combining deep technical architecture, thoughtful product design, and rigorous testing scenarios – will guide the team in delivering a successful multi-user personal data chatbot that feels as polished and insightful as an article in *The Economist*, yet is as personalised as having a private data analyst at one’s fingertips.

